{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Horses vs Humans with Convolution-improved-DNN _Computer Vision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMr50kruLTSCSSJQoFpNRUH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toluwee/Machine_Learning_Projects/blob/master/Horses_vs_Humans_with_Convolution_improved_DNN__Computer_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QkLGIN250tc"
      },
      "source": [
        "# Classifying Horses and Humans with CNN Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlMGoZB0DqKq"
      },
      "source": [
        "### Project Overview\n",
        "\n",
        "I created an image classification application using a deep neural network with convolutions. This application trains a deep learning model on a dataset of images. It then uses the trained model to classify new images.\n",
        "\n",
        "### Key Skills Demonstrated:\n",
        "\n",
        "* Tensorflow and neural networks\n",
        "* Model validation and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGrOtBu4Yn6G"
      },
      "source": [
        "### About dataset\n",
        "\n",
        "Horses or Humans is a dataset of 300Ã—300 images that contains 500 rendered images of various species of horse in various poses in various locations. It also contains 527 rendered images of humans in various poses and locations. Emphasis has been taken to ensure diversity of humans, and to that end there are both men and women as well as Asian, Black, South Asian and Caucasians present in the training set. The validation set adds 6 different figures of different gender, race and pose to ensure breadth of data.\n",
        "\n",
        "More information about the dataset available [here](http://www.laurencemoroney.com/horses-or-humans-dataset/)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUmKRHu2Yvhy"
      },
      "source": [
        "## Objective \n",
        "\n",
        "To create a model that correct classifies images as horses or humans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwTVcQyzY-8z"
      },
      "source": [
        "## Methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxHO0g3IZLPA"
      },
      "source": [
        "### Import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFp7sbu8QiCX"
      },
      "source": [
        "Dataset is loaded from Laurence Moroney's file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s1xJ0JbQZg9"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AdR3RR1Qe37"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMJ01saaZFRr"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADIW5zVWQw2h"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBhQ_vksQp6"
      },
      "source": [
        "## Dataset definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dE4LaaVQ_35"
      },
      "source": [
        "Directories containing the necessary files are referenced as such:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90cexRgOROMB"
      },
      "source": [
        "# Directory with training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with training horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with training human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE59JoulRj7g"
      },
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "\n",
        "validation_horse_hames = os.listdir(validation_horse_dir)\n",
        "validation_human_names = os.listdir(validation_human_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVMCyMzSRml3"
      },
      "source": [
        "## Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP84j9i_60jy"
      },
      "source": [
        "To define the model: \n",
        "*  Convolutional layers are added \n",
        "* Final result flattened to feed into the densely connected layers.\n",
        "* Densely connected layers added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smio3Ayq6tty"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAXLD0jgaze2"
      },
      "source": [
        "The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3SdBmfRblRY"
      },
      "source": [
        "Since this is a binary classification, sigmoid activation and binary_crossentropy loss are used so that the network output will be a single scaler between 0 and 1.\n",
        "\n",
        "RMSprop optimization algorithm is used  because it automates the learning-rate tuning. (Adam and Adagrad too will work fine because they also automatically adapt the learning rate during training.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRU55JS7bzNk"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZZ0sAU2b5Gs"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fhDAP8ifs4"
      },
      "source": [
        "Data generators that will read pictures in the source folders, convert them to float32 tensors, and feed them (with their labels) to the network are set up. \n",
        "\n",
        "Images are preprocessed  by normalizing the pixel values to be in the [0, 1] range (from the [0, 255] range).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh52a_x1i3FM"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj_tbosalK0W"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv2NGY8jn9nW"
      },
      "source": [
        "A code to initiate callback immediately a level of accuracy is achieved is used in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_My3k1E06SbL"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.90):\n",
        "      print(\"\\nReached 90% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "callbacks = myCallback()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_8fDmlxl7cP"
      },
      "source": [
        "The model is trained with the generated dateset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u721PqgIlVyy"
      },
      "source": [
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=8, \n",
        "                    epochs=15, \n",
        "                    verbose=1,\n",
        "                    validation_data = validation_generator,\n",
        "                    validation_steps=8, \n",
        "                    callbacks=[callbacks]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVe5iQU-mH-1"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnUMykalmKqB"
      },
      "source": [
        "The model is used to predict images that are uploaded "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmjCNLMgmSZb"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzLGb-hPORt_"
      },
      "source": [
        "## Clean Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dLPqbjXmzFL"
      },
      "source": [
        "The following cell is run to terminate the kernel and free memory resources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm8q_l3Wmw-9"
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}